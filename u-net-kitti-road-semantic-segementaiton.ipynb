{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from PIL import Image\nimport cv2\nimport numpy as np\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport shutil\nimport random\n\nimport matplotlib.pyplot as plt\nimport os \n\nimport torch \nimport torch.nn as nn\n\nimport time\n\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\n\nfrom sklearn.model_selection import train_test_split\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!wget https://s3.eu-central-1.amazonaws.com/avg-kitti/data_semantics.zip","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!unzip /kaggle/working/data_semantics.zip","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip install ultralytics torch opencv-python numpy\n!pip install tqdm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.listdir(\"/kaggle/working/training/semantic_rgb\")[:10]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# EDA of the images and the Labels","metadata":{}},{"cell_type":"code","source":"img = Image.open(\"/kaggle/working/training/image_2/000115_10.png\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(15, 30))\nplt.imshow(img)\nplt.axis(\"off\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Labels","metadata":{}},{"cell_type":"code","source":"label_img = Image.open(\"/kaggle/working/training/semantic_rgb/000115_10.png\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"label_img_numpy = np.array(label_img)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Reshape to (num_pixels, 3)\npixels = label_img_numpy.reshape(-1, 3)\n\n# Get unique RGB triplets\nunique_colors = np.unique(pixels, axis=0)\n\nprint(\"Unique colors (labels):\", unique_colors)\nprint(\"Number of labels:\", len(unique_colors))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"There are {len(unique_colors)} unique labels in this specific image\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### In this specific image there are 22 labels in this image!","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15, 30))\nplt.imshow(label_img_numpy)\nplt.axis(\"off\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Get the number of all labels ","metadata":{}},{"cell_type":"code","source":"\n\n# # Path to semantic RGB masks\n# mask_dir = Path(\"/kaggle/working/training/semantic_rgb\")\n\n# # Extract unique colors\n# colors = set()\n# for mask_path in tqdm(mask_dir.glob(\"*.png\"), desc=\"Scanning masks\"):\n#     mask = cv2.imread(str(mask_path))[:, :, ::-1]  # BGR → RGB\n#     unique_colors = np.unique(mask.reshape(-1, 3), axis=0)\n#     colors.update(map(tuple, unique_colors))\n\n# # Sort colors and show first 20 for reference\n# colors = sorted(colors)\n# print(f\"✅ Found {len(colors)} unique colors.\")\n# print(colors)\n\ncolors = [(0, 0, 0), (0, 0, 70), (0, 0, 90), (0, 0, 110), (0, 0, 142), (0, 0, 230), \n          (0, 60, 100), (0, 80, 100), (70, 70, 70), (70, 130, 180), (81, 0, 81), \n          (102, 102, 156), (107, 142, 35), (111, 74, 0), (119, 11, 32), (128, 64, 128),\n          (150, 100, 100), (150, 120, 90), (152, 251, 152), (153, 153, 153), \n          (180, 165, 180), (190, 153, 153), (220, 20, 60), (220, 220, 0), \n          (230, 150, 140), (244, 35, 232), (250, 170, 30), (250, 170, 160), \n          (255, 0, 0)]\n# Save color map to file\ncolor_map_file = Path(\"/kaggle/working/color_map.txt\")\nwith open(color_map_file, \"w\") as f:\n    for i, c in enumerate(colors):\n        f.write(f\"{i}: {c}\\n\")\n\nprint(f\"Color map saved to {color_map_file}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Define the UNet","metadata":{}},{"cell_type":"code","source":"class DoubleConv(nn.Module):\n    def __init__(self, in_ch, out_ch, kernel_size=3):\n        super().__init__()\n        padding = (kernel_size - 1) // 2\n        self.net = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, kernel_size, padding=padding), \n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, kernel_size, padding=padding), \n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.net(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class UNet(nn.Module):\n    def __init__(self, n_classes=1):\n        super().__init__()\n        self.d1 = DoubleConv(3, 64)\n        self.d2 = DoubleConv(64, 128)\n        self.d3 = DoubleConv(128, 256)\n        self.u1 = DoubleConv(256+128, 128)\n        self.u2 = DoubleConv(128+64, 64)\n        self.out_conv = nn.Conv2d(64, n_classes, 1)\n        self.pool = nn.MaxPool2d(2)\n        self.up = nn.Upsample(scale_factor = 2, mode = \"bilinear\", align_corners=True)\n        \n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        # print(x.shape)\n        x1 = self.d1(x)\n        # print(x1.shape)\n        x2 = self.d2(self.pool(x1))\n        # print(x2.shape)\n        x3 = self.d3(self.pool(x2))\n        # print(x3.shape)\n        x = self.up(x3)\n        # print(x.shape)\n        x = self.u1(torch.cat([x, x2], dim=1))\n        # print(x.shape)\n        x = self.up(x)\n        x = self.u2(torch.cat([x, x1], dim=1))\n        out = self.out_conv(x)\n        # print(out.shape)\n        return out","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# from torch.utils.data import Dataset\n# from PIL import Image\n# import numpy as np\n\n# # 1️⃣ Load the color map from your file\n# color_map_file = \"/kaggle/working/color_map.txt\"\n# color_to_class = {}\n\n# with open(color_map_file, \"r\") as f:\n#     for line in f:\n#         line = line.strip()\n#         if line:\n#             cls, rgb = line.split(\":\")\n#             cls = int(cls.strip())\n#             rgb = rgb.strip()[1:-1]  # remove parentheses\n#             r, g, b = [int(x) for x in rgb.split(\",\")]\n#             color_to_class[(r, g, b)] = cls\n\n# # 2️⃣ Dataset class\n# class SegmentationDataset(Dataset):\n#     def __init__(self, image_paths, label_paths, transform=None):\n#         self.image_paths = image_paths\n#         self.label_paths = label_paths\n#         self.transform = transform\n\n#     def __len__(self):\n#         return len(self.image_paths)\n\n#     def __getitem__(self, idx):\n#         # Load image\n#         img = Image.open(self.image_paths[idx]).convert(\"RGB\")\n#         # img = np.array(img)\n#         # Load RGB mask\n#         label_rgb = Image.open(self.label_paths[idx]).convert(\"RGB\")\n#         # label_rgb = np.array(label_rgb)\n\n#         if self.transform:\n#             img = self.transform[0](img)\n#             label_rgb = self.transform[1](label_rgb)\n#         # Convert RGB mask to class indices\n#         label = np.zeros((label_rgb.shape[0], label_rgb.shape[1]), dtype=np.int64)\n#         for color, cls_idx in color_to_class.items():\n#             mask = np.all(label_rgb == color, axis=-1)\n#             label[mask] = cls_idx\n\n#         # label = torch.from_numpy(label)\n#         # img = torch.from_numpy(img)\n#         return img, label\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def rgb_to_class(mask, colors):\n    mask = np.array(mask)  # H x W x 3\n    out = np.zeros((mask.shape[0], mask.shape[1]), dtype=np.int64)\n    for idx, color in enumerate(colors):\n        matches = np.all(mask == color, axis=-1)\n        out[matches] = idx\n    return out  # shape H x W","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SegmentationDataset(torch.utils.data.Dataset):\n    def __init__(self, image_paths, mask_paths, colors, img_transform=None, mask_transform=None):\n        self.image_paths = image_paths\n        self.mask_paths = mask_paths\n        self.colors = colors\n        self.img_transform = img_transform\n        self.mask_transform = mask_transform\n\n    def __getitem__(self, idx):\n        img = Image.open(self.image_paths[idx]).convert(\"RGB\")\n        mask = Image.open(self.mask_paths[idx]).convert(\"RGB\")  # keep RGB\n\n        if self.img_transform:\n            img = self.img_transform(img)\n        if self.mask_transform:\n            mask = self.mask_transform(mask)\n\n        mask = rgb_to_class(mask, self.colors)  # H x W\n        mask = torch.from_numpy(mask).long()  # 1 x H x W for DiceLoss\n        return img, mask\n\n    def __len__(self):\n        return len(self.image_paths)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_transformation = transforms.Compose([\n    transforms.Resize((368, 1224), interpolation=transforms.InterpolationMode.BILINEAR),\n    transforms.ToTensor()\n])\nlabel_transformation = transforms.Compose([\n    transforms.Resize((368, 1224), interpolation=transforms.InterpolationMode.NEAREST),\n    # transforms.ToTensor()\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_files = os.listdir(\"/kaggle/working/training/image_2\")\nlabel_files = os.listdir(\"/kaggle/working/training/semantic_rgb\")\n\nimg_files.sort()\nlabel_files.sort()\n\nimg_files = [os.path.join(\"/kaggle/working/training/image_2\", elem) for elem in img_files]\nlabel_files = [os.path.join(\"/kaggle/working/training/semantic_rgb\", elem) for elem in label_files]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Split into train and test (e.g., 80% train, 20% test)\ntrain_imgs, test_imgs, train_labels, test_labels = train_test_split(\n    img_files,\n    label_files,\n    test_size=0.2,\n    random_state=42\n)\n\nprint(f\"Train size: {len(train_imgs)}, Test size: {len(test_imgs)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_sem_seg_dataset = SegmentationDataset(train_imgs, train_labels, colors, image_transformation, label_transformation)\ntrain_loader = DataLoader(train_sem_seg_dataset, batch_size=2, shuffle=True)\n\ntest_sem_seg_dataset = SegmentationDataset(test_imgs, test_labels, colors, image_transformation, label_transformation)\nval_loader = DataLoader(test_sem_seg_dataset, batch_size=2, shuffle=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"i=0\nfor img, label in train_sem_seg_dataset:\n    i += 1 \n    print(\"Image\")\n    print(type(img))\n    print(img.shape)\n    \n    print(\"label\")\n    print(type(label))\n    print(label.shape)\n    print(torch.unique(label))\n    \n    if i > 10:\n        break\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass MultiClassDiceLoss(nn.Module):\n    def __init__(self, num_classes, smooth=1.0):\n        super().__init__()\n        self.num_classes = num_classes\n        self.smooth = smooth\n\n    def forward(self, pred, target):\n        \"\"\"\n        pred: [N, C, H, W], raw logits from model\n        target: [N, H, W], integer class labels 0..C-1\n        \"\"\"\n        pred = F.softmax(pred, dim=1)  # softmax across channels\n        target_one_hot = F.one_hot(target.long(), num_classes=self.num_classes)  # [N,H,W,C]\n        target_one_hot = target_one_hot.permute(0,3,1,2).float()  # [N,C,H,W]\n\n        intersection = (pred * target_one_hot).sum(dim=(0,2,3))  # per class\n        union = pred.sum(dim=(0,2,3)) + target_one_hot.sum(dim=(0,2,3))\n        dice = (2. * intersection + self.smooth) / (union + self.smooth)\n        loss = 1.0 - dice.mean()\n        return loss\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# class DiceLoss(nn.Module):\n#     def __init__(self):\n#         super().__init__()\n\n#     def forward(self, pred, target):\n#         pred = torch.sigmoid(pred)\n#         smooth = 1.0\n#         # print(f\"pred shape {pred.shape}, target shape {target.shape}\")\n#         intersection = (pred * target).sum()\n#         dice = (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n#         return 1.0 - dice\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# criterion = nn.CrossEntropyLoss()\n# criterion = DiceLoss()\ncriterion = MultiClassDiceLoss(29)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_epoch(model, loader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0\n    for imgs, masks in loader:\n        imgs, masks = imgs.to(device), masks.to(device).long()\n        # masks = masks.unsqueeze(1)  # Shape [B,1,H,W]\n        \n        optimizer.zero_grad()\n        outputs = model(imgs)\n        loss = criterion(outputs, masks)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    \n    return total_loss / len(loader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def validate(model, loader, criterion, device):\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for imgs, masks in loader:\n            imgs, masks = imgs.to(device), masks.to(device)\n            # masks = masks.unsqueeze(1)\n            outputs = model(imgs)\n            loss = criterion(outputs, masks)\n            total_loss += loss.item()\n    return total_loss / len(loader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = UNet(n_classes=29)\nmodel.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sum(p.numel() for p in model.parameters() if p.requires_grad)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epochs = 10\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for imgs, masks in train_loader:\n    print(imgs.shape, imgs.dtype)   # [N,3,368,1224], float32\n    print(masks.shape, masks.dtype) # [N,368,1224], int64\n    break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_val_loss = float('inf')\nfor epoch in range(epochs):\n    t0 = time.time()\n    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n    val_loss = validate(model, val_loader, criterion, device)\n   \n    if val_loss < best_val_loss:\n        print(\"Best Result! Model will be saved\")\n        best_val_loss = val_loss\n        torch.save(model.state_dict(), \"model.pt\")\n\n        \n    t1 = time.time()\n    elapsed_time = t1 - t0\n    print(f\"Epoch {epoch+1:02d}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Time: {elapsed_time:.2f}s\")\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sum(p.numel() for p in model.parameters() if p.requires_grad)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = UNet(n_classes=29).to(device)\nmodel.load_state_dict(torch.load(\"model.pt\"))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Suppose you have your color map as a NumPy array: (29, 3)\n# color_map[i] = [R, G, B] for class i\ncolor_map = np.array([  # example, replace with your real values\n    [0, 0, 0], [0, 0, 70], [0, 0, 90], [0, 0, 110], [0, 0, 142],\n    [0, 0, 230], [0, 60, 100], [0, 80, 100], [70, 70, 70], [70, 130, 180],\n    [81, 0, 81], [102, 102, 156], [107, 142, 35], [111, 74, 0], [119, 11, 32],\n    [128, 64, 128], [150, 100, 100], [150, 120, 90], [152, 251, 152], [153, 153, 153],\n    [180, 165, 180], [190, 153, 153], [220, 20, 60], [220, 220, 0], [230, 150, 140],\n    [244, 35, 232], [250, 170, 30], [250, 170, 160], [255, 0, 0]\n])\n\nmodel.eval()\nwith torch.no_grad():\n    for imgs, masks in val_loader:\n\n        imgs, masks = imgs.to(device), masks.to(device)\n        preds = model(imgs)  # (B, 29, H, W)\n\n        # Get predicted class index at each pixel\n        preds_classes = torch.argmax(preds, dim=1)  # (B, H, W)\n\n        # Convert class indices to RGB for visualization\n        preds_rgb = color_map[preds_classes.cpu().numpy()]  # shape (B, H, W, 3)\n        masks_rgb = color_map[masks.cpu().numpy()]          # shape (B, H, W, 3)\n\n        # Visualize one example\n        idx = 0  # pick first in batch\n        img_np = imgs[idx].cpu().numpy().transpose(1, 2, 0)\n        mask_np = masks_rgb[idx]\n        pred_np = preds_rgb[idx]\n\n        plt.figure(figsize=(9, 10))\n        plt.subplot(3, 1, 1)\n        plt.imshow(img_np)\n        plt.title(\"Input Image\")\n        plt.axis('off')\n\n        plt.subplot(3, 1, 2)\n        plt.imshow(mask_np.astype(np.uint8))\n        plt.title(\"Ground Truth\")\n        plt.axis('off')\n\n        plt.subplot(3, 1, 3)\n        plt.imshow(pred_np.astype(np.uint8))\n        plt.title(\"Prediction\")\n        plt.axis('off')\n\n        plt.tight_layout()\n        plt.show()\n        break\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}